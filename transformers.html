

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Transformers &mdash; pliers 0.4.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Graphs" href="graphs.html" />
    <link rel="prev" title="Stims" href="stimuli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> pliers
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installing pliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic-concepts.html">Basic concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="stimuli.html">Stims</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#extractors">Extractors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#list-of-extractor-classes">List of Extractor classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#working-with-extractor-results">Working with Extractor results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#converters">Converters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#implicit-stim-conversion">Implicit Stim conversion</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#package-wide-conversion-defaults">Package-wide conversion defaults</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#list-of-converter-classes">List of Converter classes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#filters">Filters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#list-of-filter-classes">List of Filter classes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#iterable-aware-transformations">Iterable-aware transformations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="graphs.html">Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="config.html">Configuring pliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Pliers API Reference</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pliers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Transformers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/transformers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="transformers">
<h1>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline">¶</a></h1>
<p>As the name suggests, a <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> is a kind of object that transforms other objects. In pliers, every <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> always takes a single <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> as its input, though it can return different outputs. The <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> API in pliers is modeled loosely on the widely-used scikit-learn API; as such, what defines a <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code>, from a user’s perspective, is that one can always call pass a <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> instance to <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code>’s .transform() method and expect to get another object as a result.</p>
<p>In practice, most users should never have any reason to directly instantiate the base <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> class. We will almost invariably work with one of three different <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> sub-classes: <code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code>, and <code class="xref py py-class docutils literal notranslate"><span class="pre">Filter</span></code>. These classes are distinguished by the type of output that their respective <code class="code py python docutils literal notranslate"><span class="operator"><span class="pre">.</span></span><span class="name"><span class="pre">transform</span></span><span class="punctuation"><span class="pre">()</span></span></code> methods produce:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 46%" />
<col style="width: 14%" />
<col style="width: 41%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Transformer class</p></th>
<th class="head"><p>Input</p></th>
<th class="head"><p>Output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Extractor</p></td>
<td><p>AStim</p></td>
<td><p>ExtractorResult</p></td>
</tr>
<tr class="row-odd"><td><p>Converter</p></td>
<td><p>AStim</p></td>
<td><p>BStim</p></td>
</tr>
<tr class="row-even"><td><p>Filter</p></td>
<td><p>AStim</p></td>
<td><p>AStim</p></td>
</tr>
</tbody>
</table>
<p>Here, AStim and BStim are different <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> subclasses. So an <code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code> always returns an <code class="xref py py-class docutils literal notranslate"><span class="pre">ExtractorResult</span></code>, no matter what type of <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> it receives as input. A <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">Filter</span></code> are distinguished by the fact that a <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> always returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> of a different class than its input, while a <code class="xref py py-class docutils literal notranslate"><span class="pre">Filter</span></code> always returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> of the same type as its input. This simple hierarchy turns out to be extremely powerful, as it enables us to operate in a natural, graph-like way over <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code>s, by filtering and converting them as needed before applying one or more Extractors to obtain extracted feature values.</p>
<p>Let’s examine each of these <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> types more carefully.</p>
<div class="section" id="extractors">
<h2>Extractors<a class="headerlink" href="#extractors" title="Permalink to this headline">¶</a></h2>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code>s are the most important kind of <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> in pliers, and in many cases, users will never have to touch any other kind of <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> directly. Every <code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code> implements a <code class="code py python docutils literal notranslate"><span class="name"><span class="pre">transform</span></span><span class="punctuation"><span class="pre">()</span></span></code> method that takes a <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> object as its first argument, and returns an object of class <code class="xref py py-class docutils literal notranslate"><span class="pre">ExtractorResult</span></code> (see below). For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Google Cloud Vision API face detection</span>
<span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="kn">import</span> <span class="n">GoogleVisionAPIFaceExtractor</span>

<span class="n">ext</span> <span class="o">=</span> <span class="n">GoogleVisionAPIExtractor</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s1">&#39;my_image.jpg&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="list-of-extractor-classes">
<h3>List of Extractor classes<a class="headerlink" href="#list-of-extractor-classes" title="Permalink to this headline">¶</a></h3>
<p>At present, pliers implements several dozen <code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code> classes that span a wide variety of input modalities and types of extracted features. These include:</p>
<p><strong>Audio feature extractors</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">AudiosetLabelExtractor</span></code>([hop_size, top_n, …])</p></td>
<td><p>Extract probability of 521 audio event classes based on AudioSet corpus using a YAMNet architecture.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BeatTrackExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Dynamic programming beat tracker (beat_track) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ChromaCENSExtractor</span></code>([n_chroma])</p></td>
<td><p>Extracts a chroma variant “Chroma Energy Normalized” (CENS) chromogram from audio (via Librosa).</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ChromaCQTExtractor</span></code>([n_chroma])</p></td>
<td><p>Extracts a constant-q chromogram from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ChromaSTFTExtractor</span></code>([n_chroma])</p></td>
<td><p>Extracts a chromagram from an audio’s waveform using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">HarmonicExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the harmonic elements from an audio time-series using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MeanAmplitudeExtractor</span></code>([name])</p></td>
<td><p>Mean amplitude extractor for blocks of audio with transcription.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MelspectrogramExtractor</span></code>([n_mels])</p></td>
<td><p>Extracts mel-scaled spectrogram from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MFCCExtractor</span></code>([n_mfcc])</p></td>
<td><p>Extracts Mel Frequency Ceptral Coefficients from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnsetDetectExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Detects the basic onset (onset_detect) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnsetStrengthMultiExtractor</span></code>([feature, …])</p></td>
<td><p>Computes the spectral flux onset strength envelope across multiple channels (onset_strength_multi) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PercussiveExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the percussive elements from an audio time-series using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PolyFeaturesExtractor</span></code>([order])</p></td>
<td><p>Extracts the coefficients of fitting an nth-order polynomial to the columns of an audio’s spectrogram (via Librosa).</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">RMSExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts root mean square (RMS) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpectralCentroidExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the spectral centroids from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpectralBandwidthExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the p’th-order spectral bandwidth from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpectralContrastExtractor</span></code>([n_bands])</p></td>
<td><p>Extracts the spectral contrast from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpectralFlatnessExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Computes the spectral flatness from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpectralRolloffExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the roll-off frequency from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">STFTAudioExtractor</span></code>([frame_size, hop_size, …])</p></td>
<td><p>Short-time Fourier Transform extractor.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TempogramExtractor</span></code>([win_length])</p></td>
<td><p>Extracts a tempogram from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TempoExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Detects the tempo (tempo) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TonnetzExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the tonal centroids (tonnetz) from audio using the Librosa library.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ZeroCrossingRateExtractor</span></code>([feature, hop_length])</p></td>
<td><p>Extracts the zero-crossing rate of audio using the Librosa library.</p></td>
</tr>
</tbody>
</table>
<p><strong>Image feature extractors</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BrightnessExtractor</span></code>([name])</p></td>
<td><p>Gets the average luminosity of the pixels in the image</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ClarifaiAPIImageExtractor</span></code>([api_key, model, …])</p></td>
<td><p>Uses the Clarifai API to extract tags of images.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ClarifaiAPIVideoExtractor</span></code>([api_key, model, …])</p></td>
<td><p>Uses the Clarifai API to extract tags from videos.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">FaceRecognitionFaceEncodingsExtractor</span></code>(…)</p></td>
<td><p>Uses the face_recognition package to extract a 128-dimensional encoding for every face detected in an image.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">FaceRecognitionFaceLandmarksExtractor</span></code>(…)</p></td>
<td><p>Uses the face_recognition package to extract the locations of named features of faces in the image.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">FaceRecognitionFaceLocationsExtractor</span></code>(…)</p></td>
<td><p>Uses the face_recognition package to extract bounding boxes for all faces in an image.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPIFaceExtractor</span></code>([…])</p></td>
<td><p>Identifies faces in images using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPILabelExtractor</span></code>([…])</p></td>
<td><p>Labels objects in images using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPIPropertyExtractor</span></code>([…])</p></td>
<td><p>Extracts image properties using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPISafeSearchExtractor</span></code>([…])</p></td>
<td><p>Extracts safe search detection using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPIWebEntitiesExtractor</span></code>([…])</p></td>
<td><p>Extracts web entities using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftAPIFaceExtractor</span></code>([face_id, …])</p></td>
<td><p>Extracts face features (location, emotion, accessories, etc.).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftAPIFaceEmotionExtractor</span></code>([face_id, …])</p></td>
<td><p>Extracts facial emotions from images using the Microsoft API</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPIExtractor</span></code>([features, …])</p></td>
<td><p>Base MicrosoftVisionAPIExtractor class.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPITagExtractor</span></code>([…])</p></td>
<td><p>Extracts image tags using the Microsoft API</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPICategoryExtractor</span></code>([…])</p></td>
<td><p>Extracts image categories using the Microsoft API</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPIImageTypeExtractor</span></code>([…])</p></td>
<td><p>Extracts image types (clipart, etc.) using the Microsoft API</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPIColorExtractor</span></code>([…])</p></td>
<td><p>Extracts image color attributes using the Microsoft API</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftVisionAPIAdultExtractor</span></code>([…])</p></td>
<td><p>Extracts the presence of adult content using the Microsoft API</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SaliencyExtractor</span></code>([name])</p></td>
<td><p>Determines the saliency of the image using Itti &amp; Koch (1998) algorithm implemented in pySaliencyMap</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SharpnessExtractor</span></code>([name])</p></td>
<td><p>Gets the degree of blur/sharpness of the image</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TFHubExtractor</span></code>(url_or_path[, features, …])</p></td>
<td><p>A generic class for Tensorflow Hub extractors :param url_or_path: url or path to TFHub model. You can                     browse models at <a class="reference external" href="https://tfhub.dev/">https://tfhub.dev/</a>. :type url_or_path: str :param features: list of labels (for classification)                  or other feature names. The number of items must                  match the number of features in the output. For example,                  if a classification model with 1000 output classes is passed                  (e.g. EfficientNet B6,                  see <a class="reference external" href="https://tfhub.dev/tensorflow/efficientnet/b6/classification/1">https://tfhub.dev/tensorflow/efficientnet/b6/classification/1</a>),                  this must be a list containing 1000 items. If a text encoder                  outputting 768-dimensional encoding is passed (e.g. base BERT),                  this must be a list containing 768 items. Each dimension in the                  model output will be returned as a separate feature in the                  ExtractorResult. Alternatively, the model output can be packed into a single                  feature (i.e. a vector) by passing a single-element list                  (e.g. [‘encoding’]) or a string. Along the lines of                  the previous examples, if a single feature name is                  passed here (e.g. if features=[‘encoding’]) for a TFHub model                  that outputs a 768-dimensional encoding, the extractor will                  return only one feature named ‘encoding’, which contains the                  encoding vector as a 1-d array wrapped in a list. If no value is passed, the extractor will automatically                  compute the number of features in the model output                  and return an equal number of features in pliers, labeling                  each feature with a generic prefix + its positional index                  in the model output (feature_0, feature_1, … ,feature_n). :type features: optional :param transform_out: function to transform model                       output for compatibility with extractor result :type transform_out: optional :param transform_inp: function to transform Stim.data                       for compatibility with model input format :type transform_inp: optional :param kwargs: arguments to hub.KerasLayer call :type kwargs: dict.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VibranceExtractor</span></code>([name])</p></td>
<td><p>Gets the variance of color channels of the image</p></td>
</tr>
</tbody>
</table>
<p><strong>Text feature extractors</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BertExtractor</span></code>([pretrained_model, tokenizer, …])</p></td>
<td><p>Returns encodings from the last hidden layer of BERT or similar models (ALBERT, DistilBERT, RoBERTa, CamemBERT). Excludes special tokens. Base class for other Bert extractors. :param pretrained_model: A string specifying which transformer                          model to use. Can be any pretrained BERT or BERT-derived (ALBERT,                          DistilBERT, RoBERTa, CamemBERT etc.) models listed at                          <a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">https://huggingface.co/transformers/pretrained_models.html</a>                          or path to custom model. :type pretrained_model: str :param tokenizer: Type of tokenization used in the tokenization step. If different from model, out-of-vocabulary tokens may be treated                   as unknown tokens. :type tokenizer: str :param model_class: Specifies model type. Must be one of ‘AutoModel’                     (encoding extractor) or  ‘AutoModelWithLMHead’ (language model). These are generic model classes, which use the value of                     pretrained_model to infer the model-specific transformers                     class (e.g. BertModel or BertForMaskedLM for BERT, RobertaModel                     or RobertaForMaskedLM for RoBERTa). Fixed by each subclass. :type model_class: str :param framework: name deep learning framework to use. Must be ‘pt’                   (PyTorch) or ‘tf’ (tensorflow). Defaults to ‘pt’. :type framework: str :param return_input: if True, the extractor returns encoded token                      and encoded word as features. :type return_input: bool :param model_kwargs: Named arguments for transformer model. See <a class="reference external" href="https://huggingface.co/transformers/main_classes/model.html">https://huggingface.co/transformers/main_classes/model.html</a> :type model_kwargs: dict :param tokenizer_kwargs: Named arguments for tokenizer. See <a class="reference external" href="https://huggingface.co/transformers/main_classes/tokenizer.html">https://huggingface.co/transformers/main_classes/tokenizer.html</a> :type tokenizer_kwargs: dict.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BertSequenceEncodingExtractor</span></code>([…])</p></td>
<td><p>Extract contextualized sequence encodings using pretrained BERT</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BertLMExtractor</span></code>([pretrained_model, …])</p></td>
<td><p>Returns masked words predictions from BERT (or similar, e.g.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">BertSentimentExtractor</span></code>([pretrained_model, …])</p></td>
<td><p>Extracts sentiment for sequences using BERT (or similar, e.g.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ComplexTextExtractor</span></code>([name])</p></td>
<td><p>Base ComplexTextStim Extractor class; all subclasses can only be applied to ComplexTextStim instance.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">DictionaryExtractor</span></code>(dictionary[, variables, …])</p></td>
<td><p>A generic dictionary-based extractor that supports extraction of arbitrary features contained in a lookup table.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">LengthExtractor</span></code>([name])</p></td>
<td><p>Extracts the length of the text in characters.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">NumUniqueWordsExtractor</span></code>([tokenizer])</p></td>
<td><p>Extracts the number of unique words used in the text.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PartOfSpeechExtractor</span></code>([batch_size])</p></td>
<td><p>Tags parts of speech in text with nltk.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PredefinedDictionaryExtractor</span></code>(variables[, …])</p></td>
<td><p>A generic Extractor that maps words onto values via one or more pre-defined dictionaries accessed via the web.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">SpaCyExtractor</span></code>([extractor_type, features, model])</p></td>
<td><p>A generic class for Spacy Text extractors</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TextVectorizerExtractor</span></code>([vectorizer])</p></td>
<td><p>Uses a scikit-learn Vectorizer to extract bag-of-features from text.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VADERSentimentExtractor</span></code>()</p></td>
<td><p>Uses nltk’s VADER lexicon to extract (0.0-1.0) values for the positve, neutral, and negative sentiment of a TextStim.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">WordCounterExtractor</span></code>([case_sensitive, log_scale])</p></td>
<td><p>Extracts number of times each unique word has occurred within text</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">WordEmbeddingExtractor</span></code>(embedding_file[, …])</p></td>
<td><p>An extractor that uses a word embedding file to look up embedding vectors for text.</p></td>
</tr>
</tbody>
</table>
<p><strong>Video feature extractors</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">FarnebackOpticalFlowExtractor</span></code>([pyr_scale, …])</p></td>
<td><p>Extracts total amount of dense optical flow between every pair of video frames.</p></td>
</tr>
</tbody>
</table>
<p>** Deep Learning Models **</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorFlowKerasApplicationExtractor</span></code>([…])</p></td>
<td><p>Labels objects in images using a pretrained Inception V3 architecture implemented in TensorFlow / Keras.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TFHubImageExtractor</span></code>(url_or_path[, features, …])</p></td>
<td><p>TFHub Extractor class for image models :param url_or_path: url or path to TFHub model :type url_or_path: str :param features: list of labels (for classification)                  or other feature names. If not specified, returns                  numbered features (feature_0, feature_1, … ,feature_n) :type features: optional :param rescale_rgb: whether to rescale values to 0-1 range :type rescale_rgb: bool :param reshape_input: if input needs to be reshaped,                       specifies target shape (height, width, n_channels). Details on whether the model only accept a fixed size are                       usually provided on the TFHub model page :type reshape_input: tuple :param kwargs: arguments to hub.KerasLayer call :type kwargs: dict.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TFHubTextExtractor</span></code>(url_or_path[, features, …])</p></td>
<td><p>TFHub extractor class for text models :param url_or_path: url or path to TFHub model. You can                     browse models at <a class="reference external" href="https://tfhub.dev/">https://tfhub.dev/</a>. :type url_or_path: str :param features: list of labels or other feature names. The number of items must  match the number of features                  in the model output. For example, if a text encoder                  outputting 768-dimensional encoding is passed                  (e.g. base BERT), this must be a list containing 768 items. Each dimension in the model output will be returned as a                  separate feature in the ExtractorResult. Alternatively, the model output can be packed into a single                  feature (i.e. a vector) by passing a single-element list                  (e.g. [‘encoding’]) or a string. If no value is passed,                  the extractor will automatically compute the number of                  features in the model output and return an equal number                  of features in pliers, labeling each feature with a                  generic prefix + its positional index in the model                  output (feature_0, feature_1, … ,feature_n). :type features: optional :param output_key: key to desired embedding in output                    dictionary (see documentation at                    <a class="reference external" href="https://www.tensorflow.org/hub/common_saved_model_apis/text">https://www.tensorflow.org/hub/common_saved_model_apis/text</a>). Set to None is the output is not a dictionary. :type output_key: str :param preprocessor_url_or_path: if the model requires                                  preprocessing through another TFHub model, specifies the                                  url or path to the preprocessing module. Information on                                  required preprocessing and appropriate models is generally                                  available on the TFHub model webpage :type preprocessor_url_or_path: str :param preprocessor_kwargs: dictionary or named arguments                             for preprocessor model hub.KerasLayer call :type preprocessor_kwargs: dict.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TFHubExtractor</span></code>(url_or_path[, features, …])</p></td>
<td><p>A generic class for Tensorflow Hub extractors :param url_or_path: url or path to TFHub model. You can                     browse models at <a class="reference external" href="https://tfhub.dev/">https://tfhub.dev/</a>. :type url_or_path: str :param features: list of labels (for classification)                  or other feature names. The number of items must                  match the number of features in the output. For example,                  if a classification model with 1000 output classes is passed                  (e.g. EfficientNet B6,                  see <a class="reference external" href="https://tfhub.dev/tensorflow/efficientnet/b6/classification/1">https://tfhub.dev/tensorflow/efficientnet/b6/classification/1</a>),                  this must be a list containing 1000 items. If a text encoder                  outputting 768-dimensional encoding is passed (e.g. base BERT),                  this must be a list containing 768 items. Each dimension in the                  model output will be returned as a separate feature in the                  ExtractorResult. Alternatively, the model output can be packed into a single                  feature (i.e. a vector) by passing a single-element list                  (e.g. [‘encoding’]) or a string. Along the lines of                  the previous examples, if a single feature name is                  passed here (e.g. if features=[‘encoding’]) for a TFHub model                  that outputs a 768-dimensional encoding, the extractor will                  return only one feature named ‘encoding’, which contains the                  encoding vector as a 1-d array wrapped in a list. If no value is passed, the extractor will automatically                  compute the number of features in the model output                  and return an equal number of features in pliers, labeling                  each feature with a generic prefix + its positional index                  in the model output (feature_0, feature_1, … ,feature_n). :type features: optional :param transform_out: function to transform model                       output for compatibility with extractor result :type transform_out: optional :param transform_inp: function to transform Stim.data                       for compatibility with model input format :type transform_inp: optional :param kwargs: arguments to hub.KerasLayer call :type kwargs: dict.</p></td>
</tr>
</tbody>
</table>
<p>** Misc-type extractor **</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MetricExtractor</span></code>([functions, var_names, …])</p></td>
<td><p>Extracts summary metrics from SeriesStim using numpy, scipy or custom</p></td>
</tr>
</tbody>
</table>
<p>Note that, in practice, the number of features one can extract using the above classes is extremely large, because many of these Extractors return open-ended feature sets that are determined by the contents of the input <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> and/or the specified initialization arguments. For example, most of the image-labeling Extractors that rely on deep learning-based services (e.g., <code class="xref py py-class docutils literal notranslate"><span class="pre">GoogleVisionAPILabelExtractor</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">ClarifaiAPIImageExtractor</span></code>) will return feature information for any of the top N objects detected in the image. And the <code class="xref py py-class docutils literal notranslate"><span class="pre">PredefinedDictionaryExtractor</span></code> provides a standardized interface to a large number of online word lookup dictionaries (e.g., word norms for written frequency, age-of-acquisition, emotionality ratings, etc.).</p>
</div>
<div class="section" id="working-with-extractor-results">
<h3>Working with Extractor results<a class="headerlink" href="#working-with-extractor-results" title="Permalink to this headline">¶</a></h3>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtractorResult</span></code> classes differ from other Transformers in an important way: they return feature data rather than <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> objects. Pliers imposes a standardized representation on these results; in particular, calling <code class="docutils literal notranslate"><span class="pre">transform</span></code> on any <code class="xref py py-class docutils literal notranslate"><span class="pre">Extractor</span></code> returns an aptly-named object of class <code class="xref py py-class docutils literal notranslate"><span class="pre">ExtractorResult</span></code>. This object contains all kinds of useful internal references and logged data; however, it can also be easily converted to a pandas <code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>. There’s much more to say about feature extraction results in pliers, but to keep things focused, we’ll say it in a separate <a class="reference internal" href="results.html#results"><span class="std std-ref">Results</span></a> section rather than here.</p>
</div>
</div>
<div class="section" id="converters">
<h2>Converters<a class="headerlink" href="#converters" title="Permalink to this headline">¶</a></h2>
<p>Converters, as their name suggests, convert <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> classes from one type to another. For example, the <code class="xref py py-class docutils literal notranslate"><span class="pre">IBMSpeechAPIConverter</span></code>, which is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">AudioToTextConverter</span></code>, takes an <code class="xref py py-class docutils literal notranslate"><span class="pre">AudioStim</span></code> as input, queries IBM’s Watson speech-to-text API, and returns a transcription of the audio as a <code class="xref py py-class docutils literal notranslate"><span class="pre">ComplexTextStim</span></code> object. Most <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> classes have sensible names that clearly indicate what they do, but to prevent any ambiguity (and support type-checking), every concrete <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> class must define <code class="code py python docutils literal notranslate"><span class="name"><span class="pre">_input_type</span></span></code> and <code class="code py python docutils literal notranslate"><span class="name"><span class="pre">_output_type</span></span></code> properties that indicate what <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> classes they take and return as input and output, respectively.</p>
<div class="section" id="implicit-stim-conversion">
<h3>Implicit Stim conversion<a class="headerlink" href="#implicit-stim-conversion" title="Permalink to this headline">¶</a></h3>
<p>Although Converters play a critical role in pliers, they usually don’t need to be invoked explicitly by users, as pliers can usually figure out what conversions must be performed and carry them out implicitly. For example, suppose we want to run the <code class="xref py py-class docutils literal notranslate"><span class="pre">STFTAudioExtractor</span></code>—which computes the short-time Fourier transform on an audio clip and returns its power spectrum—on the audio track of a movie clip. We don’t need to explicitly convert the <code class="xref py py-class docutils literal notranslate"><span class="pre">VideoStim</span></code> to an <code class="xref py py-class docutils literal notranslate"><span class="pre">AudioStim</span></code>, because pliers is clever enough to determine that it can get the appropriate input for the <code class="xref py py-class docutils literal notranslate"><span class="pre">STFTAudioExtractor</span></code> by executing the <code class="xref py py-class docutils literal notranslate"><span class="pre">VideoToAudioConverter</span></code>. In practice, then, the following two snippets produce identical results:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="kn">import</span> <span class="n">STFTAudioExtractor</span>
<span class="kn">from</span> <span class="nn">pliers.stimuli</span> <span class="kn">import</span> <span class="n">VideoStim</span>
<span class="n">video</span> <span class="o">=</span> <span class="n">VideoStim</span><span class="p">(</span><span class="s1">&#39;my_movie.mp4&#39;</span><span class="p">)</span>

<span class="c1"># Option A: explicit conversion</span>
<span class="kn">from</span> <span class="nn">pliers.converters</span> <span class="kn">import</span> <span class="n">VideoToAudioConverter</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">VideoToAudioConverter</span><span class="p">()</span>
<span class="n">audio</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">STFTAudioExtractor</span><span class="p">(</span><span class="n">freq_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

<span class="c1"># Option B: implicit conversion</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">STFTAudioExtractor</span><span class="p">(</span><span class="n">freq_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
</pre></div>
</div>
<p>Because pliers contains a number of “multistep” <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> classes, which chain together multiple standard Converters, implicit <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> conversion will typically work not only for a single conversion, but also for a whole series of them. For example, if you feed a video file to a <code class="xref py py-class docutils literal notranslate"><span class="pre">LengthExtractor</span></code> (which just counts the number of characters in each TextStim’s text), pliers will use the built-in VideoToTextConverter class to transform your <code class="xref py py-class docutils literal notranslate"><span class="pre">VideoStim</span></code> into a <code class="xref py py-class docutils literal notranslate"><span class="pre">TextStim</span></code>, and everything should work smoothly in most cases.</p>
<p>I say “most” cases, because there are two important gotchas to be aware of when relying on implicit conversion. First, sometimes there’s an inherent ambiguity about what trajectory a given stimulus should take through converter space; in such cases, the default conversions pliers performs may not line up with your expectations. For example, a <code class="xref py py-class docutils literal notranslate"><span class="pre">VideoStim</span></code> can be converted to a <code class="xref py py-class docutils literal notranslate"><span class="pre">TextStim</span></code> either by (a) extracting the audio track from the video and then transcribing into text via a speech recognition service, or (b) extracting the video frames from the video and then attempting to detect any text labels within each image. Because pliers has no way of knowing which of these you’re trying to accomplish, it will default to the first. The upshot is that if you think there’s any chance of ambiguity in the conversion process, it’s probably a good idea to explicitly chain the <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> steps (you can do this very easily using the <code class="xref py py-class docutils literal notranslate"><span class="pre">Graph</span></code> interface discussed separately). The explicit approach also provides additional precision in that you may want to initialize a particular <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> with non-default arguments, and/or specify exactly which of several candidate <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> classes to use (e.g., pliers defaults to performing speech-to-text conversion via the IBM Watson API, but also provides alternative support for the Wit.AI, and Google Cloud Speech APIs services).</p>
<div class="section" id="package-wide-conversion-defaults">
<span id="conversion-defaults"></span><h4>Package-wide conversion defaults<a class="headerlink" href="#package-wide-conversion-defaults" title="Permalink to this headline">¶</a></h4>
<p>Alternatively, you can set the default Converter(s) to use for any implicit <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> conversion at a package-wide level, via the config.default_converters attribute. By default, this is something like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">default_converters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;AudioStim-&gt;TextStim&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;IBMSpeechAPIConverter&#39;</span><span class="p">,</span> <span class="s1">&#39;WitTranscriptionConverter&#39;</span><span class="p">),</span>
    <span class="s1">&#39;ImageStim-&gt;TextStim&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;GoogleVisionAPITextConverter&#39;</span><span class="p">,</span> <span class="s1">&#39;TesseractConverter&#39;</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Here, each entry in the default_converters dictionary lists the Converter(s) to use, in order of preference. For example, the above indicates that any conversion between <code class="xref py py-class docutils literal notranslate"><span class="pre">ImageStim</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">TextStim</span></code> should first try to use the <code class="xref py py-class docutils literal notranslate"><span class="pre">GoogleVisionAPITextConverter</span></code>, and then, if that fails (e.g., because the user has no Google Cloud Vision API key set up), fall back on the <code class="xref py py-class docutils literal notranslate"><span class="pre">TesseractConverter</span></code>. If all selections specified in the config fail, pliers will still try to use any matching Converters it finds, but you’ll lose the ability to control the order of selection.</p>
<p>Second, because many Converters call API-based services, if you’re going to rely on implicit conversion, you should make sure that any API keys you might need are properly set up as environment variables in your local environment, seeing as you’re not going to be able to pass those keys to the Converter as initialization arguments. For example, by default, pliers uses the IBM Watson API for speech-to-text conversion (i.e., when converting an <code class="xref py py-class docutils literal notranslate"><span class="pre">AudioStim</span></code> to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ComplexTextStim</span></code>). But since you won’t necessarily know this ahead of time, you won’t be able to initialize the Converter with the correct credentials–i.e., by calling <code class="code py python docutils literal notranslate"><span class="name"><span class="pre">IBMSpeechAPIConverter</span></span><span class="punctuation"><span class="pre">(</span></span><span class="name"><span class="pre">username</span></span><span class="operator"><span class="pre">=</span></span><span class="literal string single"><span class="pre">’my_username’</span></span><span class="punctuation"><span class="pre">,</span></span> <span class="name"><span class="pre">password</span></span><span class="operator"><span class="pre">=</span></span><span class="literal string single"><span class="pre">’my_password’</span></span><span class="punctuation"><span class="pre">)</span></span></code>. Instead, the Converter will get initialized without any arguments (<code class="code py python docutils literal notranslate"><span class="name"><span class="pre">IBMSpeechAPIConverter</span></span><span class="punctuation"><span class="pre">()</span></span></code>), which means the initialization logic will immediately proceed to look for IBM_USERNAME and IBM_PASSWORD variables in the environment, and will raise an exception if at least one of these variables is missing. So make sure as many API keys as possible are appropriately set in the environment. You can read more about this in the API keys section.</p>
</div>
</div>
<div class="section" id="list-of-converter-classes">
<h3>List of Converter classes<a class="headerlink" href="#list-of-converter-classes" title="Permalink to this headline">¶</a></h3>
<p>Pliers currently implements the following <code class="xref py py-class docutils literal notranslate"><span class="pre">Converter</span></code> classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ComplexTextIterator</span></code>([name])</p></td>
<td><p>Iterates elements in a ComplexTextStim as TextStims.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtractorResultToSeriesConverter</span></code>([name])</p></td>
<td><p>Converts an ExtractorResult instance to a list of SeriesStims.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">IBMSpeechAPIConverter</span></code>([username, password, …])</p></td>
<td><p>Uses the IBM Watson Text to Speech API to run speech-to-text transcription on an audio file.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleSpeechAPIConverter</span></code>([language_code, …])</p></td>
<td><p>Uses the Google Speech API to do speech-to-text transcription.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">GoogleVisionAPITextConverter</span></code>([…])</p></td>
<td><p>Detects text within images using the Google Cloud Vision API.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">MicrosoftAPITextConverter</span></code>([language, …])</p></td>
<td><p>Detects text within images using the Microsoft Vision API.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">RevAISpeechAPIConverter</span></code>([access_token, …])</p></td>
<td><p>Uses the Rev AI speech-to-text API to transcribe an audio file.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TesseractConverter</span></code>([name])</p></td>
<td><p>Uses the Tesseract library to extract text from images.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoFrameCollectionIterator</span></code>([name])</p></td>
<td><p>Iterates frames in a DerivedVideoStim as ImageStims.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoFrameIterator</span></code>([name])</p></td>
<td><p>Iterates frames in a VideoStim as ImageStims.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoToAudioConverter</span></code>([name])</p></td>
<td><p>Convert a VideoStim to an AudioStim by extracting the audio track using moviepy.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoToComplexTextConverter</span></code>([steps])</p></td>
<td><p>Converts a VideoStim directly to a ComplexTextStim.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoToTextConverter</span></code>([steps])</p></td>
<td><p>Converts a VideoStim directly to a TextStim.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">WitTranscriptionConverter</span></code>([api_key, rate_limit])</p></td>
<td><p>Speech-to-text transcription via the Wit.ai API.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtractorResultToSeriesConverter</span></code>([name])</p></td>
<td><p>Converts an ExtractorResult instance to a list of SeriesStims.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="filters">
<h2>Filters<a class="headerlink" href="#filters" title="Permalink to this headline">¶</a></h2>
<p>A <code class="code py python docutils literal notranslate"><span class="name"><span class="pre">Filter</span></span></code> is a kind of <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> that returns an object of the same <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> class as its input. Filters can be used for tasks like image or audio filtering, text tokenization or sanitization, and many other things. The defining feature of a <code class="xref py py-class docutils literal notranslate"><span class="pre">Filter</span></code> class is simply that it must return a <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> of the same type as the input passed to the <code class="code py python docutils literal notranslate"><span class="operator"><span class="pre">.</span></span><span class="name"><span class="pre">transform</span></span><span class="punctuation"><span class="pre">()</span></span></code> method (e.g., passing in an <code class="xref py py-class docutils literal notranslate"><span class="pre">ImageStim</span></code> and getting back another, modified, <code class="xref py py-class docutils literal notranslate"><span class="pre">ImageStim</span></code>).</p>
<div class="section" id="list-of-filter-classes">
<h3>List of Filter classes<a class="headerlink" href="#list-of-filter-classes" title="Permalink to this headline">¶</a></h3>
<p>Pliers currently implements the following <code class="xref py py-class docutils literal notranslate"><span class="pre">Filter</span></code> classes:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">AudioTrimmingFilter</span></code>([start, end, frames, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">FrameSamplingFilter</span></code>([every, hertz, top_n])</p></td>
<td><p>Samples frames from video stimuli, to improve efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">ImageCroppingFilter</span></code>([box])</p></td>
<td><p>Crops an image.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">LowerCasingFilter</span></code>([name])</p></td>
<td><p>Lower cases the text in a TextStim.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PillowImageFilter</span></code>([image_filter])</p></td>
<td><p>Uses the ImageFilter module from PIL to run a pre-defined image enhancement filter on an ImageStim.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">PunctuationRemovalFilter</span></code>([name])</p></td>
<td><p>Removes punctuation from a TextStim.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TemporalTrimmingFilter</span></code>([start, end, frames, …])</p></td>
<td><p>Temporally trims the contents of the audio stimulus using the provided start and end points.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TokenizingFilter</span></code>([tokenizer])</p></td>
<td><p>Tokenizes a TextStim into several word TextStims.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">TokenRemovalFilter</span></code>([tokens, language])</p></td>
<td><p>Removes tokens (e.g., stopwords, common words, punctuation) from a TextStim.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">VideoTrimmingFilter</span></code>([start, end, frames, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">WordStemmingFilter</span></code>([stemmer, tokenize, …])</p></td>
<td><p>Nltk-based word stemming and lemmatization Filter.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">AudioResamplingFilter</span></code>([target_sr, resample_type])</p></td>
<td><p>Librosa-based audio resampling Filter.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="iterable-aware-transformations">
<h2>Iterable-aware transformations<a class="headerlink" href="#iterable-aware-transformations" title="Permalink to this headline">¶</a></h2>
<p>A useful feature of the <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> API is that it’s inherently iterable-aware: every pliers <code class="xref py py-class docutils literal notranslate"><span class="pre">Transformer</span></code> (including all Extractors, Converters, and Filters) can be passed an iterable (specifically, a list, tuple, or generator) of <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code> objects rather than just a single <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code>. The transformation will then be applied independently to each <code class="xref py py-class docutils literal notranslate"><span class="pre">Stim</span></code>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="graphs.html" class="btn btn-neutral float-right" title="Graphs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="stimuli.html" class="btn btn-neutral float-left" title="Stims" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017, Developers of pliers.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>